{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ce63ad9d-1b4b-4501-84c7-5b89b22017b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d74a95b-063b-4582-8cd2-4354c5555598",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset parquet (/home/deema/.cache/huggingface/datasets/arbml___parquet/arbml--alpagasus_cleaned_ar-493077b64f717191/0.0.0/2a3b91fbd88a2c90d1dbbb32b460cf621d31bd5b05b934492fdef7d8d6f236ec)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5644c7d8c6a408898ef85baeb5363d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset(\"arbml/alpagasus_cleaned_ar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6df112bf-33b9-4ea4-88e0-c312b371b4d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.remove_columns(['instruction_en', 'output_en'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bb2dfd7-d015-4495-8215-bc84cf4d5a4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f49a7db7b1445f6b01e4d518eebdc7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/9229 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = dataset.map(lambda batch: {'input': [None] * len(batch['index'])}, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f6dd381a-8cf0-4925-98d5-9791601d64b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['instruction', 'output', 'index', 'input'],\n",
       "        num_rows: 9229\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "85122eb9-4a23-4ecc-a774-3b1f4ddcb9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'instruction': [' أعطني ثلاث نصائح للحفاظ على الصحة.\\n',\n",
       "  ' كيف يمكننا تقليل التلوث الهوائي؟\\n',\n",
       "  ' أنشئ قائمة بالصفات التي تصف الشخص الشجاع.\\n',\n",
       "  ' قدم تفسيرًا للسبب والنتيجة للعبارة التالية. أدى الوباء إلى زيادة كبيرة في التسوق عبر الإنترنت.\\n',\n",
       "  ' ابحث عن عاصمة كندا. أخرج اسم المدينة.\\n'],\n",
       " 'output': [' 1. تناول نظام غذائي متوازن وتأكد من تضمين الكثير من الفواكه والخضروات.\\n2. ممارسة التمارين الرياضية بانتظام للحفاظ على نشاط وقوة الجسم.\\n3. الحصول على قسط كافٍ من النوم والحفاظ على جدول نوم منتظم.',\n",
       "  ' هناك عدة طرق لتقليل التلوث الهوائي، مثل الانتقال إلى مصادر الطاقة المتجددة، وتشجيع استخدام وسائل النقل العام، ومنع حرق الوقود الأحفوري، وتنفيذ سياسات لتقليل الانبعاثات من المصادر الصناعية، وتنفيذ معايير انبعاثات المركبات. بالإضافة إلى ذلك، يمكن للأفراد أن يقوموا بدورهم في تقليل التلوث الهوائي عن طريق تقليل استخدام السيارات، وتجنب حرق المواد مثل الخشب، والتحول إلى أجهزة كهربائية فعالة من حيث استخدام الطاقة.',\n",
       "  ' شجاع، بطل، جريء، قوي، شجاع، حازم، جريء، شجاع، جريء، عنيد، شجاع، قوي القلب، شجاع، شجاع، لا يخشى، شجاع، لا يرتعب، مفعم بالحيوية، مجازف.',\n",
       "  ' تسبب الوباء في إجبار الناس على البقاء في المنزل لتجنب التعرض المحتمل للفيروس، مما أدى إلى زيادة التسوق عبر الإنترنت. مع زيادة عدد الأشخاص الذين يتسوقون عبر الإنترنت، زاد الطلب على المشتريات عبر الإنترنت واستجابت الشركات بتقديم المزيد من الخيارات عبر الإنترنت. ونتيجة لذلك، زادت النشاطات عبر الإنترنت بشكل كبير.',\n",
       "  ' أوتاوا'],\n",
       " 'index': [0, 1, 2, 3, 4],\n",
       " 'input': [None, None, None, None, None]}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c2ff3c3c-1e79-40a0-a3b0-e4300dd504ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_to_save = dataset['train']\n",
    "\n",
    "data_as_list = [item for item in data_to_save]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "371c9492-bde1-4fb1-9055-23c19dc6a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file_path = 'alpagasus_cleaned_ar.json'\n",
    "\n",
    "with open(json_file_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(data_as_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9c21b3a1-fd90-4d5b-9edc-f789a88e8e91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to alpagasus_cleaned_ar.json\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset saved to {json_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc9d42a6-11ee-470c-85ef-59b09b943cb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting safetensors\n",
      "  Downloading safetensors-0.4.1-cp39-cp39-manylinux_2_17_ppc64le.manylinux2014_ppc64le.whl (1.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "peft 0.7.2.dev0 requires torch>=1.13.0, but you have torch 1.12.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed safetensors-0.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install safetensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d787923-012f-4721-8964-779f099b3052",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting fire\n",
      "  Using cached fire-0.5.0.tar.gz (88 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /opt/miniconda3/envs/deepcam/lib/python3.8/site-packages (from fire) (1.15.0)\n",
      "Requirement already satisfied: termcolor in /opt/miniconda3/envs/deepcam/lib/python3.8/site-packages (from fire) (1.1.0)\n",
      "Building wheels for collected packages: fire\n",
      "  Building wheel for fire (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for fire: filename=fire-0.5.0-py2.py3-none-any.whl size=116950 sha256=eb6f3d81dd40477d419b1c0cb96784f2fec5ce387b5bb1d4bed59c734c369d39\n",
      "  Stored in directory: /home/deema/.cache/pip/wheels/5b/eb/43/7295e71293b218ddfd627f935229bf54af9018add7fbb5aac6\n",
      "Successfully built fire\n",
      "Installing collected packages: fire\n",
      "Successfully installed fire-0.5.0\n"
     ]
    }
   ],
   "source": [
    "! pip install fire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87e3a1d4-6f3b-4ced-ae68-e170aafe1a49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"finetune.py\", line 7, in <module>\n",
      "    import transformers\n",
      "  File \"/home/deema/.local/lib/python3.8/site-packages/transformers/__init__.py\", line 26, in <module>\n",
      "    from . import dependency_versions_check\n",
      "  File \"/home/deema/.local/lib/python3.8/site-packages/transformers/dependency_versions_check.py\", line 16, in <module>\n",
      "    from .utils.versions import require_version, require_version_core\n",
      "  File \"/home/deema/.local/lib/python3.8/site-packages/transformers/utils/__init__.py\", line 31, in <module>\n",
      "    from .generic import (\n",
      "  File \"/home/deema/.local/lib/python3.8/site-packages/transformers/utils/generic.py\", line 432, in <module>\n",
      "    import torch.utils._pytree as _torch_pytree\n",
      "ModuleNotFoundError: No module named 'torch.utils._pytree'\n"
     ]
    }
   ],
   "source": [
    "!python finetune.py \\\n",
    "    --base_model 'FreedomIntelligence/AceGPT-7B' \\\n",
    "    --data_path 'alpagasus_cleaned_ar.json' \\\n",
    "    --output_dir './lora-alpaca_alpagasus'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6270cc4f-e256-4c6d-b54d-8b7c6300a5c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in /opt/miniconda3/envs/opence-v1.7.2/lib/python3.9/site-packages (1.12.1)\n",
      "Requirement already satisfied: torchvision in /opt/miniconda3/envs/opence-v1.7.2/lib/python3.9/site-packages (0.13.1)\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement torchaudio (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for torchaudio\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06c637e8-9d21-4a35-b66f-ee776896801e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "\n",
      "===================================BUG REPORT===================================\n",
      "/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: Welcome to bitsandbytes. For bug reports, please run\n",
      "\n",
      "python -m bitsandbytes\n",
      "\n",
      "\n",
      "  warn(msg)\n",
      "================================================================================\n",
      "/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/opt/miniconda3/envs/opence-v1.9.1/lib/libcudart.so.11.0'), PosixPath('/opt/miniconda3/envs/opence-v1.9.1/lib/libcudart.so')}.. We select the PyTorch default libcudart.so, which is {torch.version.cuda},but this might missmatch with the CUDA version that is needed for bitsandbytes.To override this behavior set the BNB_CUDA_VERSION=<version string, e.g. 122> environmental variableFor example, if you want to use the CUDA version 122BNB_CUDA_VERSION=122 python ...OR set the environmental variable in your .bashrc: export BNB_CUDA_VERSION=122In the case of a manual override, make sure you set the LD_LIBRARY_PATH, e.g.export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.2\n",
      "  warn(msg)\n",
      "/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: /opt/miniconda3/envs/opence-v1.9.1 did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/usr/local/cuda-11.4/extras/CUPTI/lib64')}\n",
      "/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda-11.4/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda-11.4/lib64/libcudart.so')}.. We select the PyTorch default libcudart.so, which is {torch.version.cuda},but this might missmatch with the CUDA version that is needed for bitsandbytes.To override this behavior set the BNB_CUDA_VERSION=<version string, e.g. 122> environmental variableFor example, if you want to use the CUDA version 122BNB_CUDA_VERSION=122 python ...OR set the environmental variable in your .bashrc: export BNB_CUDA_VERSION=122In the case of a manual override, make sure you set the LD_LIBRARY_PATH, e.g.export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.2\n",
      "  warn(msg)\n",
      "/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: /usr/local/cuda-11.4/extras/CUPTI/lib64:/usr/local/cuda-11.4/lib64:/opt/openmpi-v4.1.1/lib:/opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/site-packages/nvidia/dali:/opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/site-packages/tensorflow did not contain ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] as expected! Searching further paths...\n",
      "  warn(msg)\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1'), PosixPath('1;/home/deema/.local/bin'), PosixPath('1;/opt/swsuite/bin'), PosixPath('1;/home/deema/bin'), PosixPath('1;/usr/local/sbin'), PosixPath('1;/usr/sbin'), PosixPath('1;/usr/share/Modules/bin'), PosixPath('1;/usr/local/bin'), PosixPath('1;/usr/bin'), PosixPath('1;/opt/openmpi-v4.1.1/bin'), PosixPath('1;/home/deema/miniconda3/condabin')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/var/www/ood/apps/sys/dashboard')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1;/opt/openmpi-v4.1.1/lib'), PosixPath('1'), PosixPath('/usr/local/cuda-11.4/extras/CUPTI/lib64'), PosixPath('1;/usr/local/cuda-11.4/lib64')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('1'), PosixPath('1;/opt/modulefiles/mldl'), PosixPath('1;/opt/modulefiles/system'), PosixPath('1;/usr/share/modulefiles/Linux'), PosixPath('1;/opt/modulefiles/compiler'), PosixPath('1;/opt/modulefiles/software'), PosixPath('1;/opt/modulefiles/conda'), PosixPath('1;/usr/share/modulefiles/Core'), PosixPath('1;/usr/share/lmod/lmod/modulefiles/Core'), PosixPath('1;/opt/modulefiles/mpi')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('sys/dashboard/sys/jupyter-lab')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('conda_base'), PosixPath('cuda/11.4.48'), PosixPath('StdEnv'), PosixPath('swsuite/0.4'), PosixPath('openmpi/4.1.1'), PosixPath('jupyter/4.7.1')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('module'), PosixPath('//matplotlib_inline.backend_inline')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('/usr/share/modulefiles/Linux'), PosixPath('/usr/share/modulefiles/Core'), PosixPath('/opt/modulefiles/mldl'), PosixPath('/opt/modulefiles/software')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath(\"() {  tr -cd 'a-zA-Z0-9' < /dev/urandom 2> /dev/null | head -c${1\"), PosixPath('-8}\\n}')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('() {  ( alias;\\n eval ${which_declare} ) | /usr/bin/which --tty-only --read-alias --read-functions --show-tilde --show-dot $@\\n}')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('-30}\";\\n for ((i=1; i<=time*2; i++))\\n do\\n port_used \"${port}\";\\n port_status=$?;\\n if [ \"$port_status\" == \"0\" ]; then\\n return 0;\\n else\\n if [ \"$port_status\" == \"127\" ]; then\\n echo \"commands to find port were either not found or inaccessible.\";\\n echo \"command options are lsof, nc, bash\\'s /dev/tcp, or python (or python3) with socket lib.\";\\n return 127;\\n fi;\\n fi;\\n sleep 0.5;\\n done;\\n return 1\\n}'), PosixPath('() {  local port=\"${1}\";\\n local time=\"${2')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('-0}\" = \\'1\\' ]; then\\n typeset swname=\\'main\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd.tcl ]; then\\n typeset swfound=0;\\n unset MODULES_USE_COMPAT_VERSION;\\n fi;\\n else\\n typeset swname=\\'compatibility\\';\\n if [ -e /usr/share/Modules/libexec/modulecmd-compat ]; then\\n typeset swfound=0;\\n MODULES_USE_COMPAT_VERSION=1;\\n export MODULES_USE_COMPAT_VERSION;\\n fi;\\n fi;\\n if [ $swfound -eq 0 ]; then\\n echo \"Switching to Modules $swname version\";\\n source /usr/share/Modules/init/bash;\\n else\\n echo \"Cannot switch to Modules $swname version, command not found\";\\n return 1;\\n fi\\n}'), PosixPath('() {  typeset swfound=1;\\n if [ \"${MODULES_USE_COMPAT_VERSION')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('() {  if [ \"$1\" = \"load\" -o \"$1\" = \"unload\" ]; then\\n eval \"module $@\";\\n else\\n /usr/bin/scl \"$@\";\\n fi\\n}')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('-8}\\n };\\n export -f create_passwd\\n}'), PosixPath('\"$2\" > /dev/null 2>&1\\n };\\n function port_used_bash () \\n { \\n local bash_supported=$(strings /bin/bash 2>/dev/null | grep tcp);\\n if [ \"$bash_supported\" == \"/dev/tcp/*/*\" ]; then\\n ( '), PosixPath('() {  function random_number () \\n { \\n shuf -i ${1}-${2} -n 1\\n };\\n export -f random_number;\\n function port_used_python () \\n { \\n python -c \"import socket; socket.socket().connect((\\'$1\\',$2))\" > /dev/null 2>&1\\n };\\n function port_used_python3 () \\n { \\n python3 -c \"import socket; socket.socket().connect((\\'$1\\',$2))\" > /dev/null 2>&1\\n };\\n function port_used_nc () \\n { \\n nc -w 2 \"$1\" \"$2\" < /dev/null > /dev/null 2>&1\\n };\\n function port_used_lsof () \\n { \\n lsof -i '), PosixPath('\\' || echo \"localhost\") | awk \\'END{print $NF}\\');\\n local port_strategies=(port_used_nc port_used_lsof port_used_bash port_used_python port_used_python3);\\n for strategy in ${port_strategies[@]};\\n do\\n $strategy $host $port;\\n status=$?;\\n if [[ \"$status\" == \"0\" ]] || [[ \"$status\" == \"1\" ]]; then\\n return $status;\\n fi;\\n done;\\n return 127\\n };\\n export -f port_used;\\n function find_port () \\n { \\n local host=\"${1'), PosixPath('-localhost}\";\\n local port=$(random_number \"${2'), PosixPath(\" '\\\\(.*\\\\)\"), PosixPath(' < /dev/tcp/$1/$2 ) > /dev/null 2>&1;\\n else\\n return 127;\\n fi\\n };\\n function port_used () \\n { \\n local port=\"${1#*'), PosixPath('${port}\"; do\\n port=$(random_number \"${2'), PosixPath('-2000}\" \"${3'), PosixPath('-65535}\");\\n done;\\n echo \"${port}\"\\n };\\n export -f find_port;\\n function wait_until_port_used () \\n { \\n local port=\"${1}\";\\n local time=\"${2'), PosixPath('-30}\";\\n for ((i=1; i<=time*2; i++))\\n do\\n port_used \"${port}\";\\n port_status=$?;\\n if [ \"$port_status\" == \"0\" ]; then\\n return 0;\\n else\\n if [ \"$port_status\" == \"127\" ]; then\\n echo \"commands to find port were either not found or inaccessible.\";\\n echo \"command options are lsof, nc, bash\\'s /dev/tcp, or python (or python3) with socket lib.\";\\n return 127;\\n fi;\\n fi;\\n sleep 0.5;\\n done;\\n return 1\\n };\\n export -f wait_until_port_used;\\n function create_passwd () \\n { \\n tr -cd \\'a-zA-Z0-9\\' < /dev/urandom 2> /dev/null | head -c${1'), PosixPath('}\";\\n local host=$((expr \"${1}\" '), PosixPath('-65535}\");\\n while port_used \"${host}')}\n",
      "The following directories listed in your path were found to be non-existent: {PosixPath('() {  eval \"$($LMOD_DIR/ml_cmd \"$@\")\"\\n}')}\n",
      "CUDA_SETUP: WARNING! libcudart.so not found in any environmental path. Searching in backup paths...\n",
      "/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: Found duplicate ['libcudart.so', 'libcudart.so.11.0', 'libcudart.so.12.0'] files: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}.. We select the PyTorch default libcudart.so, which is {torch.version.cuda},but this might missmatch with the CUDA version that is needed for bitsandbytes.To override this behavior set the BNB_CUDA_VERSION=<version string, e.g. 122> environmental variableFor example, if you want to use the CUDA version 122BNB_CUDA_VERSION=122 python ...OR set the environmental variable in your .bashrc: export BNB_CUDA_VERSION=122In the case of a manual override, make sure you set the LD_LIBRARY_PATH, e.g.export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/usr/local/cuda-11.2\n",
      "  warn(msg)\n",
      "DEBUG: Possible options found for libcudart.so: {PosixPath('/usr/local/cuda/lib64/libcudart.so.11.0'), PosixPath('/usr/local/cuda/lib64/libcudart.so')}\n",
      "CUDA SETUP: PyTorch settings found: CUDA_VERSION=118, Highest Compute Capability: 7.0.\n",
      "CUDA SETUP: To manually override the PyTorch CUDA version please see:https://github.com/TimDettmers/bitsandbytes/blob/main/how_to_use_nonpytorch_cuda.md\n",
      "/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/cuda_setup/main.py:166: UserWarning: WARNING: Compute capability < 7.5 detected! Only slow 8-bit matmul is supported for your GPU!                     If you run into issues with 8-bit matmul, you can try 4-bit quantization: https://huggingface.co/blog/4bit-transformers-bitsandbytes\n",
      "  warn(msg)\n",
      "CUDA SETUP: Loading binary /home/deema/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so...\n",
      "/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/libbitsandbytes_cuda118_nocublaslt.so: cannot open shared object file: No such file or directory\n",
      "CUDA SETUP: Something unexpected happened. Please compile from source:\n",
      "git clone https://github.com/TimDettmers/bitsandbytes.git\n",
      "cd bitsandbytes\n",
      "CUDA_VERSION=118 make cuda11x_nomatmul\n",
      "python setup.py install\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/runpy.py\", line 188, in _run_module_as_main\n",
      "    mod_name, mod_spec, code = _get_module_details(mod_name, _Error)\n",
      "  File \"/opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/runpy.py\", line 147, in _get_module_details\n",
      "    return _get_module_details(pkg_main_name, error)\n",
      "  File \"/opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/runpy.py\", line 111, in _get_module_details\n",
      "    __import__(pkg_name)\n",
      "  File \"/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/__init__.py\", line 6, in <module>\n",
      "    from . import cuda_setup, utils, research\n",
      "  File \"/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/research/__init__.py\", line 1, in <module>\n",
      "    from . import nn\n",
      "  File \"/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/research/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import LinearFP8Mixed, LinearFP8Global\n",
      "  File \"/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/research/nn/modules.py\", line 8, in <module>\n",
      "    from bitsandbytes.optim import GlobalOptimManager\n",
      "  File \"/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/optim/__init__.py\", line 6, in <module>\n",
      "    from bitsandbytes.cextension import COMPILED_WITH_CUDA\n",
      "  File \"/home/deema/.local/lib/python3.9/site-packages/bitsandbytes/cextension.py\", line 20, in <module>\n",
      "    raise RuntimeError('''\n",
      "RuntimeError: \n",
      "        CUDA Setup failed despite GPU being available. Please run the following command to get more information:\n",
      "\n",
      "        python -m bitsandbytes\n",
      "\n",
      "        Inspect the output of the command and see if you can locate CUDA libraries. You might need to add them\n",
      "        to your LD_LIBRARY_PATH. If you suspect a bug, please take the information from python -m bitsandbytes\n",
      "        and open an issue at: https://github.com/TimDettmers/bitsandbytes/issues\n"
     ]
    }
   ],
   "source": [
    "! python -m bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7331f751-2bc9-4c8a-9b98-ccf7228ca1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Dec 20 06:24:26 2023       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 545.23.08              Driver Version: 545.23.08    CUDA Version: 12.3     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla V100-SXM2-16GB           On  | 00000004:04:00.0 Off |                    0 |\n",
      "| N/A   31C    P0              37W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   1  Tesla V100-SXM2-16GB           On  | 00000004:05:00.0 Off |                    0 |\n",
      "| N/A   33C    P0              36W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   2  Tesla V100-SXM2-16GB           On  | 00000035:03:00.0 Off |                    0 |\n",
      "| N/A   30C    P0              36W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "|   3  Tesla V100-SXM2-16GB           On  | 00000035:04:00.0 Off |                    0 |\n",
      "| N/A   34C    P0              37W / 300W |      2MiB / 16384MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614ae661-4f52-4b73-beba-76beaf2d327d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing installation: torch 2.0.1\n",
      "Uninstalling torch-2.0.1:\n",
      "  Would remove:\n",
      "    /opt/miniconda3/envs/opence-v1.9.1/bin/convert-caffe2-to-onnx\n",
      "    /opt/miniconda3/envs/opence-v1.9.1/bin/convert-onnx-to-caffe2\n",
      "    /opt/miniconda3/envs/opence-v1.9.1/bin/torchrun\n",
      "    /opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/site-packages/caffe2\n",
      "    /opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/site-packages/functorch\n",
      "    /opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/site-packages/nvfuser\n",
      "    /opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/site-packages/torch\n",
      "    /opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/site-packages/torch-2.0.1-py3.9.egg-info\n",
      "    /opt/miniconda3/envs/opence-v1.9.1/lib/python3.9/site-packages/torchgen\n",
      "Proceed (Y/n)? "
     ]
    }
   ],
   "source": [
    "! pip3 uninstall torch torchvision "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f985d067-4483-4923-ae19-11661af14a11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2021 NVIDIA Corporation\n",
      "Built on Mon_Oct_11_21:54:04_PDT_2021\n",
      "Cuda compilation tools, release 11.4, V11.4.152\n",
      "Build cuda_11.4.r11.4/compiler.30521435_0\n"
     ]
    }
   ],
   "source": [
    "!nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141e5807-aa11-4d6a-80d1-a8d0f0b9546b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepcam]",
   "language": "python",
   "name": "conda-env-deepcam-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
